# Ollama Configuration
# Make sure Ollama is installed and running (ollama serve)
# Models are configured in models_config.py
# 
# To pull recommended models, run:
# ollama pull llama3.1
# ollama pull mistral
# ollama pull gemma2
# ollama pull phi3
# ollama pull qwen2.5
#
# Ollama will run on http://localhost:11434 by default